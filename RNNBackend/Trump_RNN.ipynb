{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Trump_RNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyjRO2WOlX7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDntWi6OlX76",
        "colab_type": "code",
        "outputId": "5d59e375-da84-4b51-d003-745290e8d0a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "path_to_file = tf.keras.utils.get_file('donald-tweets-clean.txt', 'https://github.com/Varun-S-Nair/Trump-Tweet-Generator/raw/master/RNNBackend/donald-tweets-clean.txt')\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 291204 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SzuzOorlX8M",
        "colab_type": "code",
        "outputId": "481932e7-6422-4571-cd30-4c5b07e9f965",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "# text = text[5:]\n",
        "print(text[:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!\n",
            "Love the fact that the small groups of protesters last night have passion for our great country. We will all come togeth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCz6PR5RlX8Q",
        "colab_type": "code",
        "outputId": "fce8b612-4547-4876-832a-bc16a6587e73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IWXo0MOlX8e",
        "colab_type": "code",
        "outputId": "3110c34b-c8ef-4354-b1af-7f62dc677447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '#' :   3,\n",
            "  '$' :   4,\n",
            "  '%' :   5,\n",
            "  '&' :   6,\n",
            "  '(' :   7,\n",
            "  ')' :   8,\n",
            "  '+' :   9,\n",
            "  ',' :  10,\n",
            "  '-' :  11,\n",
            "  '.' :  12,\n",
            "  '/' :  13,\n",
            "  '0' :  14,\n",
            "  '1' :  15,\n",
            "  '2' :  16,\n",
            "  '3' :  17,\n",
            "  '4' :  18,\n",
            "  '5' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xx6zMIt_lX8n",
        "colab_type": "code",
        "outputId": "67d65877-91ac-4136-dcbf-8963d956de6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Busy day plan' ---- characters mapped to int ---- > [29 75 73 79  1 58 55 79  1 70 66 55 68]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNkmcoaplX8x",
        "colab_type": "code",
        "outputId": "0674cefd-238d-48d4-f168-2523c32b07e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "B\n",
            "u\n",
            "s\n",
            "y\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKBn3FAwlX83",
        "colab_type": "code",
        "outputId": "1b892eb9-742c-40a6-cc16-4c8c632cc3cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#The batch method lets us easily convert these individual characters to sequences of the desired size.\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!\\nLove the fact that th'\n",
            "'e small groups of protesters last night have passion for our great country. We will all come together and be proud!\\nJust had a very open and successful'\n",
            "' presidential election. Now professional protesters, incited by the media, are protesting. Very unfair!\\nA fantastic day in D.C. Met with President Obam'\n",
            "'a for first time. Really good meeting, great chemistry. Melania liked Mrs. O a lot!\\nSuch a beautiful and important evening! The forgotten man and woman'\n",
            "' will never be forgotten again. We will all come together as never before\\nDont let up, keep getting out to vote - this election is FAR FROM OVER! We ar'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2QBMMUglX89",
        "colab_type": "code",
        "outputId": "31966461-00d2-428a-9dfd-6157fedda4b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "#For each sequence, duplicate and shift it to form the input and target text by using the map method to apply a \n",
        "#simple function to each batch:\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "#Print the first examples input and target values:\n",
        "\n",
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!\\nLove the fact that t'\n",
            "Target data: 'usy day planned in New York. Will soon be making some very important decisions on the people who will be running our government!\\nLove the fact that th'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0is0k-F7lX9C",
        "colab_type": "code",
        "outputId": "4d15ec11-e041-4e31-b23f-40e656a5057e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the \n",
        "#index for \"F\" and trys to predict the index for \"i\" as the next character. At the next timestep, it does the same \n",
        "#thing but the RNN considers the previous step context in addition to the current input character.\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 29 ('B')\n",
            "  expected output: 75 ('u')\n",
            "Step    1\n",
            "  input: 75 ('u')\n",
            "  expected output: 73 ('s')\n",
            "Step    2\n",
            "  input: 73 ('s')\n",
            "  expected output: 79 ('y')\n",
            "Step    3\n",
            "  input: 79 ('y')\n",
            "  expected output: 1 (' ')\n",
            "Step    4\n",
            "  input: 1 (' ')\n",
            "  expected output: 58 ('d')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rIjpZKllX9H",
        "colab_type": "code",
        "outputId": "32dafb82-4446-4641-a683-2bfa50a0cc09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Create training batches\n",
        "#We used tf.data to split the text into manageable sequences. But before feeding this data into the model, \n",
        "#we need to shuffle the data and pack it into batches.\n",
        "\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MVM3xqalX9L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoNUxCsAlX9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEgsSiZ1lX9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8Z0K-UclX9l",
        "colab_type": "code",
        "outputId": "4504228e-2a05-4019-bbc9-62d642c05aa0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#First check the shape of the output:\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 150, 84) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfhUM99qlX9v",
        "colab_type": "code",
        "outputId": "36f0209d-4cfe-4716-b5d2-446d898d60c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           21504     \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (64, None, 1024)          3938304   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 84)            86100     \n",
            "=================================================================\n",
            "Total params: 4,045,908\n",
            "Trainable params: 4,045,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XZwYsuVlX91",
        "colab_type": "code",
        "outputId": "3745ca03-7ef4-4b3e-d742-4e1f4d8e19a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "#To get actual predictions from the model we need to sample from the output distribution, to get actual character \n",
        "#indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "#Try it for the first example in the batch:\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "#This gives us, at each timestep, a prediction of the next character index:\n",
        "sampled_indices"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([58, 19, 15, 83, 30, 26, 83, 34, 66, 17, 80, 49, 47,  4, 80, 44, 29,\n",
              "       53, 28, 37, 30,  1, 65, 20, 41, 76, 67, 47, 76, 18, 83, 63,  1, 76,\n",
              "       26, 27,  9, 72, 81, 78, 61, 38, 59, 31, 11, 65, 72, 79, 52, 76, 21,\n",
              "       75,  5, 43, 54, 31, 35, 78, 71, 22, 57, 26, 21, 44, 71,  7, 36, 21,\n",
              "        2, 77, 36, 53, 60, 35, 75,  2, 46, 13, 30,  4, 61, 31, 15,  0, 68,\n",
              "       66, 44, 77, 51, 40,  4, 45, 26, 31,  5, 39, 80,  1, 59,  0, 62, 12,\n",
              "       74, 39,  6, 68, 21, 77, 21, 55, 11, 64, 82, 79, 58, 57, 81, 59, 37,\n",
              "       43, 11, 30,  5, 67, 39, 64, 51, 62, 15, 74, 22, 34, 47, 15, 16, 71,\n",
              "       51, 62, 14, 58, 17, 16, 63, 64, 34, 28, 58, 64, 40, 19])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKjMv3kFlX-A",
        "colab_type": "code",
        "outputId": "3d481e82-ec5f-4049-a4c9-fcfcb9d595cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#Decode these to see the text predicted by this untrained model:\n",
        "\n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'ent with great leadership skills and vision, not someone like Hillary or Barack, neither of which has a clue!\\nThe phony Club For Growth, which asked m'\n",
            "\n",
            "Next Char Predictions: \n",
            " 'd51۪C?۪Gl3zVT$zQBZAJC k6NvmTv4۪i v?@+ṟxgKeD-kryYv7u%P_DHxq8c?7Qq(I7!wIZfHu!S/C$gD1\\nnlQwXM$R?D%Lz e\\nh.tL&n7w7a-j\\u06ddydc̱eJP-C%mLjXh1t8GT12qXh0d32ijGAdjM5'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRG8L80_lX-a",
        "colab_type": "code",
        "outputId": "6c064754-5034-402e-ba28-cc5d6dd278c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Train the Model\n",
        "#At this point the problem can be treated as a standard classification problem. Given the previous RNN state, \n",
        "#and the input this time step, predict the class of the next character.\n",
        "\n",
        "#Attach an optimizer, and a loss function\n",
        "#The standard tf.keras.losses.sparse_categorical_crossentropy loss function works in this case because it is \n",
        "#applied across the last dimension of the predictions.\n",
        "\n",
        "#Because our model returns logits, we need to set the from_logits flag.\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 150, 84)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.43088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wTJltMllX-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Configure the training procedure using the tf.keras.Model.compile method. We'll use tf.keras.optimizers.Adam with \n",
        "#default arguments and the loss function.\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67-qtd9PlX-p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Configure checkpoints\n",
        "#Use a tf.keras.callbacks.ModelCheckpoint to ensure that checkpoints are saved during training:\n",
        "# Directory where the checkpoints will be saved\n",
        "!rm -rf ./training_checkpoints\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5rY1qgFlX-v",
        "colab_type": "code",
        "outputId": "3c7a48e8-f4d2-4a2b-ba81-a01eeb25e8fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Execute the training\n",
        "#To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster \n",
        "#training.\n",
        "#Loss minimized at epoch 72 (0.3808), Accuracy maximized at epoch 57/66/70\n",
        "EPOCHS=72\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 3.8055 - accuracy: 0.1276\n",
            "Epoch 2/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 2.9421 - accuracy: 0.2358\n",
            "Epoch 3/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 2.6174 - accuracy: 0.2903\n",
            "Epoch 4/72\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 2.4740 - accuracy: 0.3139\n",
            "Epoch 5/72\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 2.3764 - accuracy: 0.3378\n",
            "Epoch 6/72\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 2.2744 - accuracy: 0.3606\n",
            "Epoch 7/72\n",
            "30/30 [==============================] - 3s 89ms/step - loss: 2.1573 - accuracy: 0.3940\n",
            "Epoch 8/72\n",
            "30/30 [==============================] - 3s 90ms/step - loss: 2.0398 - accuracy: 0.4276\n",
            "Epoch 9/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 1.9267 - accuracy: 0.4601\n",
            "Epoch 10/72\n",
            "30/30 [==============================] - 3s 88ms/step - loss: 1.8199 - accuracy: 0.4906\n",
            "Epoch 11/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 1.7229 - accuracy: 0.5168\n",
            "Epoch 12/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 1.6335 - accuracy: 0.5406\n",
            "Epoch 13/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 1.5541 - accuracy: 0.5632\n",
            "Epoch 14/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 1.4835 - accuracy: 0.5822\n",
            "Epoch 15/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 1.4178 - accuracy: 0.5994\n",
            "Epoch 16/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 1.3622 - accuracy: 0.6147\n",
            "Epoch 17/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 1.3070 - accuracy: 0.6294\n",
            "Epoch 18/72\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 1.2561 - accuracy: 0.6427\n",
            "Epoch 19/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 1.2108 - accuracy: 0.6542\n",
            "Epoch 20/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 1.1638 - accuracy: 0.6668\n",
            "Epoch 21/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 1.1200 - accuracy: 0.6791\n",
            "Epoch 22/72\n",
            "30/30 [==============================] - 3s 83ms/step - loss: 1.0768 - accuracy: 0.6906\n",
            "Epoch 23/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 1.0337 - accuracy: 0.7027\n",
            "Epoch 24/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.9905 - accuracy: 0.7143\n",
            "Epoch 25/72\n",
            "30/30 [==============================] - 2s 83ms/step - loss: 0.9485 - accuracy: 0.7275\n",
            "Epoch 26/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.9024 - accuracy: 0.7424\n",
            "Epoch 27/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.8609 - accuracy: 0.7540\n",
            "Epoch 28/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.8164 - accuracy: 0.7690\n",
            "Epoch 29/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.7706 - accuracy: 0.7836\n",
            "Epoch 30/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.7271 - accuracy: 0.7981\n",
            "Epoch 31/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.6855 - accuracy: 0.8121\n",
            "Epoch 32/72\n",
            "30/30 [==============================] - 3s 84ms/step - loss: 0.6423 - accuracy: 0.8261\n",
            "Epoch 33/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.5960 - accuracy: 0.8429\n",
            "Epoch 34/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.5581 - accuracy: 0.8563\n",
            "Epoch 35/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.5169 - accuracy: 0.8708\n",
            "Epoch 36/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.4833 - accuracy: 0.8833\n",
            "Epoch 37/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.4482 - accuracy: 0.8957\n",
            "Epoch 38/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.4174 - accuracy: 0.9061\n",
            "Epoch 39/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.3846 - accuracy: 0.9167\n",
            "Epoch 40/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.3593 - accuracy: 0.9251\n",
            "Epoch 41/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.3350 - accuracy: 0.9321\n",
            "Epoch 42/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.3132 - accuracy: 0.9385\n",
            "Epoch 43/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2986 - accuracy: 0.9429\n",
            "Epoch 44/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.2833 - accuracy: 0.9473\n",
            "Epoch 45/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2711 - accuracy: 0.9504\n",
            "Epoch 46/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.2575 - accuracy: 0.9532\n",
            "Epoch 47/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.2476 - accuracy: 0.9559\n",
            "Epoch 48/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2385 - accuracy: 0.9579\n",
            "Epoch 49/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.2298 - accuracy: 0.9597\n",
            "Epoch 50/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2239 - accuracy: 0.9608\n",
            "Epoch 51/72\n",
            "30/30 [==============================] - 3s 94ms/step - loss: 0.2180 - accuracy: 0.9619\n",
            "Epoch 52/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.2137 - accuracy: 0.9631\n",
            "Epoch 53/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.2062 - accuracy: 0.9642\n",
            "Epoch 54/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.2022 - accuracy: 0.9650\n",
            "Epoch 55/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1992 - accuracy: 0.9656\n",
            "Epoch 56/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1953 - accuracy: 0.9664\n",
            "Epoch 57/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.1899 - accuracy: 0.9675\n",
            "Epoch 58/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1876 - accuracy: 0.9677\n",
            "Epoch 59/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1846 - accuracy: 0.9683\n",
            "Epoch 60/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1823 - accuracy: 0.9686\n",
            "Epoch 61/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1798 - accuracy: 0.9695\n",
            "Epoch 62/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1764 - accuracy: 0.9698\n",
            "Epoch 63/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1754 - accuracy: 0.9700\n",
            "Epoch 64/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1728 - accuracy: 0.9704\n",
            "Epoch 65/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1709 - accuracy: 0.9706\n",
            "Epoch 66/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1690 - accuracy: 0.9709\n",
            "Epoch 67/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1666 - accuracy: 0.9714\n",
            "Epoch 68/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1654 - accuracy: 0.9715\n",
            "Epoch 69/72\n",
            "30/30 [==============================] - 3s 87ms/step - loss: 0.1657 - accuracy: 0.9715\n",
            "Epoch 70/72\n",
            "30/30 [==============================] - 3s 85ms/step - loss: 0.1619 - accuracy: 0.9720\n",
            "Epoch 71/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1612 - accuracy: 0.9721\n",
            "Epoch 72/72\n",
            "30/30 [==============================] - 3s 86ms/step - loss: 0.1587 - accuracy: 0.9726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EepHI1NlX-0",
        "colab_type": "code",
        "outputId": "4f103cab-fd10-4edc-e571-d715b35502de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Generate text\n",
        "#Restore the latest checkpoint\n",
        "#To keep this prediction step simple, use a batch size of 1.\n",
        "\n",
        "#Because of the way the RNN state is passed from timestep to timestep, the model only accepts a fixed batch size \n",
        "#once built.\n",
        "\n",
        "#To run the model with a different batch_size, we need to rebuild the model and restore the weights from the \n",
        "#checkpoint.\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_72'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDuMqS6vlX-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_VovInllX-6",
        "colab_type": "code",
        "outputId": "e8b0e574-3bb9-40e4-8e27-8dccef08a5a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 256)            21504     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 84)             86100     \n",
            "=================================================================\n",
            "Total params: 4,045,908\n",
            "Trainable params: 4,045,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iko6ralylX-_",
        "colab_type": "code",
        "outputId": "276306c0-e0df-41f8-fa42-bca39b3cb735",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#The prediction loop\n",
        "#The following code block generates the text:\n",
        "\n",
        "#It Starts by choosing a start string, initializing the RNN state and setting the number of characters to generate.\n",
        "\n",
        "#Get the prediction distribution of the next character using the start string and the RNN state.\n",
        "\n",
        "#Then, use a categorical distribution to calculate the index of the predicted character. Use this predicted character \n",
        "#as our next input to the model.\n",
        "\n",
        "#The RNN state returned by the model is fed back into the model so that it now has more context, instead than only \n",
        "#one character. After predicting the next character, the modified RNN states are again fed back into the model, \n",
        "#which is how it learns as it gets more context from the previously predicted characters.\n",
        "\n",
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))\n",
        "\n",
        "print(generate_text(model, start_string=u\"M\"))\n",
        "\n",
        "#The easiest thing you can do to improve the results it to train it for longer (try EPOCHS=30).\n",
        "\n",
        "#You can also experiment with a different start string, or try adding another RNN layer to improve the model's \n",
        "#accuracy, or adjusting the temperature parameter to generate more or less random predictions."
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MERICA very dumb funny, for your kind words. We will have a do why Mitt Romney Caniflinto and she went so important--- stopping illegal immigration!\n",
            "Just got great national poll numbers. We got she hoselergust polls against me. She is a joke!\n",
            "I am not just running against Crooked Hillary Clinton doesnt have the strength or storm!\n",
            "She doesnt even look presidential!\n",
            "Crooked Hillary no longer has crowd!\n",
            "Great Trudpan control oupport!\n",
            "Watching John Kasich believe that mach that mession- he doesnt have a clue. I hear she will soon be gone!\n",
            ".@secupp, who cant believe that her husband should not be allowed in in fact. Nothonest media!\n",
            "I am at Trump National Doral-best resort in newspice any of very sad!\n",
            "Thank you Kayle Hillary Clinton cannot even bring herself to say he wants to run against me.\n",
            "Senator Liberty Instruct. See you there.\n",
            "That is happy with a chekeand that their was no ISIS vidio and he U.S. Senate. She has done nothing!\n",
            "I dont want to hit Crazing @megynkelly Stupid about them the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KHwQBTdlX_D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Advanced: Customized Training\n",
        "#The above training procedure is simple, but does not give you much control.\n",
        "\n",
        "#So now that you've seen how to run the model manually let's unpack the training loop, and implement it ourselves. \n",
        "#This gives a starting point if, for example, to implement curriculum learning to help stabilize the model's open-loop \n",
        "#output.\n",
        "\n",
        "#We will use tf.GradientTape to track the gradients. You can learn more about this approach by reading the eager \n",
        "#execution guide.\n",
        "\n",
        "#The procedure works as follows:\n",
        "\n",
        "#First, initialize the RNN state. We do this by calling the tf.keras.Model.reset_states method.\n",
        "\n",
        "#Next, iterate over the dataset (batch by batch) and calculate the predictions associated with each.\n",
        "\n",
        "#Open a tf.GradientTape, and calculate the predictions and loss in that context.\n",
        "\n",
        "#Calculate the gradients of the loss with respect to the model variables using the tf.GradientTape.grads method.\n",
        "\n",
        "#Finally, take a step downwards by using the optimizer's tf.train.Optimizer.apply_gradients method.\n",
        "\n",
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_ozBgdclX_G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, target):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inp)\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            target, predictions, from_logits=True))\n",
        "  grads = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kb_kTcMlX_J",
        "colab_type": "code",
        "outputId": "220784f9-ddbd-48d3-d6e7-f7631a795c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Training step\n",
        "EPOCHS = 72\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  # initializing the hidden state at the start of every epoch\n",
        "  # initally hidden is None\n",
        "  hidden = model.reset_states()\n",
        "\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    loss = train_step(inp, target)\n",
        "\n",
        "    if batch_n % 100 == 0:\n",
        "      template = 'Epoch {} Batch {} Loss {}'\n",
        "      print(template.format(epoch+1, batch_n, loss))\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.432528972625732\n",
            "Epoch 1 Loss 3.1484\n",
            "Time taken for 1 epoch 3.853494644165039 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 3.1502151489257812\n",
            "Epoch 2 Loss 2.7118\n",
            "Time taken for 1 epoch 2.667045831680298 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.7216367721557617\n",
            "Epoch 3 Loss 2.4943\n",
            "Time taken for 1 epoch 2.6748578548431396 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.5583674907684326\n",
            "Epoch 4 Loss 2.4216\n",
            "Time taken for 1 epoch 2.685960292816162 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.4174256324768066\n",
            "Epoch 5 Loss 2.2866\n",
            "Time taken for 1 epoch 2.726372003555298 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.2953503131866455\n",
            "Epoch 6 Loss 2.1820\n",
            "Time taken for 1 epoch 2.695415735244751 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.197901725769043\n",
            "Epoch 7 Loss 2.0779\n",
            "Time taken for 1 epoch 2.674403429031372 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.0747458934783936\n",
            "Epoch 8 Loss 1.8986\n",
            "Time taken for 1 epoch 2.7174761295318604 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.8992677927017212\n",
            "Epoch 9 Loss 1.8610\n",
            "Time taken for 1 epoch 2.7082982063293457 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.8210840225219727\n",
            "Epoch 10 Loss 1.8095\n",
            "Time taken for 1 epoch 2.7282726764678955 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.6929789781570435\n",
            "Epoch 11 Loss 1.6454\n",
            "Time taken for 1 epoch 2.7121939659118652 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.6276428699493408\n",
            "Epoch 12 Loss 1.5378\n",
            "Time taken for 1 epoch 2.690087080001831 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.5365419387817383\n",
            "Epoch 13 Loss 1.4734\n",
            "Time taken for 1 epoch 2.6952645778656006 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.4355345964431763\n",
            "Epoch 14 Loss 1.4424\n",
            "Time taken for 1 epoch 2.6897261142730713 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.4192248582839966\n",
            "Epoch 15 Loss 1.3752\n",
            "Time taken for 1 epoch 2.7083253860473633 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.371794581413269\n",
            "Epoch 16 Loss 1.3793\n",
            "Time taken for 1 epoch 2.65403151512146 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.3056600093841553\n",
            "Epoch 17 Loss 1.2970\n",
            "Time taken for 1 epoch 2.6875720024108887 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.233116865158081\n",
            "Epoch 18 Loss 1.1856\n",
            "Time taken for 1 epoch 2.6761910915374756 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.1985450983047485\n",
            "Epoch 19 Loss 1.1835\n",
            "Time taken for 1 epoch 2.6698367595672607 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.0918145179748535\n",
            "Epoch 20 Loss 1.1576\n",
            "Time taken for 1 epoch 2.712129592895508 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.051755428314209\n",
            "Epoch 21 Loss 1.0888\n",
            "Time taken for 1 epoch 2.673962116241455 sec\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.0254346132278442\n",
            "Epoch 22 Loss 1.0222\n",
            "Time taken for 1 epoch 2.654409170150757 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9790181517601013\n",
            "Epoch 23 Loss 0.9861\n",
            "Time taken for 1 epoch 2.6884706020355225 sec\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.9515181183815002\n",
            "Epoch 24 Loss 0.9604\n",
            "Time taken for 1 epoch 2.6700220108032227 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.8734729886054993\n",
            "Epoch 25 Loss 0.9119\n",
            "Time taken for 1 epoch 2.7277750968933105 sec\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.8374688625335693\n",
            "Epoch 26 Loss 0.8606\n",
            "Time taken for 1 epoch 2.669910430908203 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.7865068316459656\n",
            "Epoch 27 Loss 0.8646\n",
            "Time taken for 1 epoch 2.6528713703155518 sec\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.7131044268608093\n",
            "Epoch 28 Loss 0.8105\n",
            "Time taken for 1 epoch 2.6879944801330566 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.7034732103347778\n",
            "Epoch 29 Loss 0.7840\n",
            "Time taken for 1 epoch 2.65576171875 sec\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.6687835454940796\n",
            "Epoch 30 Loss 0.7237\n",
            "Time taken for 1 epoch 2.7197072505950928 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.6136687397956848\n",
            "Epoch 31 Loss 0.6783\n",
            "Time taken for 1 epoch 2.6947147846221924 sec\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.5654392242431641\n",
            "Epoch 32 Loss 0.6293\n",
            "Time taken for 1 epoch 2.653101682662964 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.5183041095733643\n",
            "Epoch 33 Loss 0.5826\n",
            "Time taken for 1 epoch 2.6703648567199707 sec\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.4755397439002991\n",
            "Epoch 34 Loss 0.5387\n",
            "Time taken for 1 epoch 2.691847801208496 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.4445643126964569\n",
            "Epoch 35 Loss 0.5069\n",
            "Time taken for 1 epoch 2.7063426971435547 sec\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.4005957543849945\n",
            "Epoch 36 Loss 0.4648\n",
            "Time taken for 1 epoch 2.6981163024902344 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.3599437475204468\n",
            "Epoch 37 Loss 0.4264\n",
            "Time taken for 1 epoch 2.683185577392578 sec\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.33804625272750854\n",
            "Epoch 38 Loss 0.4066\n",
            "Time taken for 1 epoch 2.6594600677490234 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.3152870833873749\n",
            "Epoch 39 Loss 0.3875\n",
            "Time taken for 1 epoch 2.68818998336792 sec\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.2853989601135254\n",
            "Epoch 40 Loss 0.3475\n",
            "Time taken for 1 epoch 2.7280306816101074 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.2716383635997772\n",
            "Epoch 41 Loss 0.3372\n",
            "Time taken for 1 epoch 2.675535202026367 sec\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.2481001317501068\n",
            "Epoch 42 Loss 0.3189\n",
            "Time taken for 1 epoch 2.6877312660217285 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.22797033190727234\n",
            "Epoch 43 Loss 0.2912\n",
            "Time taken for 1 epoch 2.672290325164795 sec\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.21670033037662506\n",
            "Epoch 44 Loss 0.2761\n",
            "Time taken for 1 epoch 2.6917262077331543 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.2089499831199646\n",
            "Epoch 45 Loss 0.2805\n",
            "Time taken for 1 epoch 2.709425210952759 sec\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.19990074634552002\n",
            "Epoch 46 Loss 0.2672\n",
            "Time taken for 1 epoch 2.678459405899048 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.18030349910259247\n",
            "Epoch 47 Loss 0.2503\n",
            "Time taken for 1 epoch 2.692474842071533 sec\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.18399234116077423\n",
            "Epoch 48 Loss 0.2440\n",
            "Time taken for 1 epoch 2.6729896068573 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.16691015660762787\n",
            "Epoch 49 Loss 0.2319\n",
            "Time taken for 1 epoch 2.6932668685913086 sec\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.16109855473041534\n",
            "Epoch 50 Loss 0.2241\n",
            "Time taken for 1 epoch 2.7300424575805664 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.14895471930503845\n",
            "Epoch 51 Loss 0.2172\n",
            "Time taken for 1 epoch 2.671097993850708 sec\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.15330861508846283\n",
            "Epoch 52 Loss 0.2109\n",
            "Time taken for 1 epoch 2.670668125152588 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.13933458924293518\n",
            "Epoch 53 Loss 0.2047\n",
            "Time taken for 1 epoch 2.691410779953003 sec\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.1429467499256134\n",
            "Epoch 54 Loss 0.2072\n",
            "Time taken for 1 epoch 2.6717679500579834 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.13709726929664612\n",
            "Epoch 55 Loss 0.2033\n",
            "Time taken for 1 epoch 2.7098042964935303 sec\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.14295096695423126\n",
            "Epoch 56 Loss 0.2022\n",
            "Time taken for 1 epoch 2.662308692932129 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.12738248705863953\n",
            "Epoch 57 Loss 0.2054\n",
            "Time taken for 1 epoch 2.6714553833007812 sec\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.13096122443675995\n",
            "Epoch 58 Loss 0.1919\n",
            "Time taken for 1 epoch 2.697420358657837 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.13042354583740234\n",
            "Epoch 59 Loss 0.1945\n",
            "Time taken for 1 epoch 2.6706702709198 sec\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.12284746766090393\n",
            "Epoch 60 Loss 0.1933\n",
            "Time taken for 1 epoch 2.7260708808898926 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.12646983563899994\n",
            "Epoch 61 Loss 0.1821\n",
            "Time taken for 1 epoch 2.692378044128418 sec\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.11616350710391998\n",
            "Epoch 62 Loss 0.1816\n",
            "Time taken for 1 epoch 2.690063953399658 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.11711663752794266\n",
            "Epoch 63 Loss 0.1802\n",
            "Time taken for 1 epoch 2.6724343299865723 sec\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.12115141749382019\n",
            "Epoch 64 Loss 0.1696\n",
            "Time taken for 1 epoch 2.6902103424072266 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.11088139563798904\n",
            "Epoch 65 Loss 0.1767\n",
            "Time taken for 1 epoch 2.729314088821411 sec\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.10845385491847992\n",
            "Epoch 66 Loss 0.1678\n",
            "Time taken for 1 epoch 2.692469596862793 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.11545051634311676\n",
            "Epoch 67 Loss 0.1720\n",
            "Time taken for 1 epoch 2.690721035003662 sec\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.10434848815202713\n",
            "Epoch 68 Loss 0.1657\n",
            "Time taken for 1 epoch 2.6676151752471924 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.10058054327964783\n",
            "Epoch 69 Loss 0.1673\n",
            "Time taken for 1 epoch 2.6963634490966797 sec\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.111730195581913\n",
            "Epoch 70 Loss 0.1733\n",
            "Time taken for 1 epoch 2.731550931930542 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.10782119631767273\n",
            "Epoch 71 Loss 0.1535\n",
            "Time taken for 1 epoch 2.6788930892944336 sec\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.10371048003435135\n",
            "Epoch 72 Loss 0.1777\n",
            "Time taken for 1 epoch 2.6867477893829346 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFuIpRtgYVM1",
        "colab_type": "code",
        "outputId": "561e9784-8c8b-4f96-d4e9-e90e849d46b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_71'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EB3qpULuYbp8",
        "colab_type": "code",
        "outputId": "bc9eb8cc-952e-46d8-8e41-b62b59b102d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 256)            21504     \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           3938304   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 84)             86100     \n",
            "=================================================================\n",
            "Total params: 4,045,908\n",
            "Trainable params: 4,045,908\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bTsm9R-YcG5",
        "colab_type": "code",
        "outputId": "c0db97b3-72d8-45a3-c8af-b7ae586443a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"S\"))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ston the other candidates.\n",
            "Just another candidates are finally  should have know that the Republicans picked Cleveland insthat Iowa Cauco dumb us all of the at four MAKE AMERICA GREAT AGAIN!\n",
            "The endorsement of me by the 11,500 Bold, Jers. How many tale-lied to the FBI and to the people of New York, and all others laughing!\n",
            "The ratings for the Republican Party that are currently and selfishly opposed to the Fadellary Wadly, they are not about me run against Crooked Hillary Clinton than protected to explain away the new Monmouth poll 41 to 14 of the media hit job. Leading Ohio 48 - 44.\n",
            "Voter Trump University case but the preds where I have a waste of time being interviewed by @FoxNews was a disgrace to good broadcasting and journalism. Who would ever had, to announce this Salama Sanders is her way up and people who control our politicians (puppets) are spending the way for many great Supreme Cooright on @FoxNews Tony. I am with you all the way! Make America Great Again!\n",
            "Rany people have g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCj_Zjg4YZbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}